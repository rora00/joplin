id: 5184b2985ed44275b8958a5fdb84f2b4
parent_id: 9857682ef9894927ba8cca329457390c
item_type: 1
item_id: 989faf8706574a4b8719e7d3764c1473
item_updated_time: 1590489882731
title_diff: 
body_diff: "@@ -1761,215 +1761,8 @@\n e.%0A%0A\n-## Bias term%0AThere is an additional bias term %0A$$%0Ax_o=1%0A$$%0Awhich is independent to the inputs and is needed to shift the activation function (explained later) to the left or right regardless of the inputs.%0A%0A\n ## H\n@@ -2093,16 +2093,23 @@\n h input \n+neuron \n connects\n@@ -2113,21 +2113,24 @@\n cts \n-to one\n+an\n  output\n+ neuron\n . If\n@@ -2167,16 +2167,39 @@\n diction \n+for that output neuron \n is calcu\n@@ -2247,156 +2247,873 @@\n %7Dx_i\n+w_i\n %0A$$%0A%0A## \n-Activation function%0AOnce all the neurons and their weights are summed up, they pass through an activation function. The activation function \n+Bias term%0AThere is an additional bias term %0A$$%0Ax_o=1%0A$$%0Awhich is independent to the inputs and is needed to shift the activation function (explained later) to the left or right regardless of the inputs. Thus, %0A%0A$$%0A%5Chat%7By%7D=w_0+%5Csum_%7Bi=1%7D%5E%7Bn%7Dx_iw_i%0A$$%0A%0A## Non-linear activation function%0AOnce all the neurons and their weights are summed up, they pass through an activation function. The activation function can convert the prediction into something useful to be used later.%0A%0AActivation functions are non linear because the equation so far produces a linear equation%0A%0AA common non-linear activation function is the sigmoid function. This converts any real number into a probability between 0 and 1. Thus,%0A%0A$$%0A%5Chat%7By%7D=g(w_0+%5Csum_%7Bi=1%7D%5E%7Bn%7Dx_iw_i)%0A$$%0A%0AOther non-linear activation functions include%0A1. Hyperbolic tangent (tanh)%0A2. Rectified linear unit (relu)%0A%0A\n %0A%0A%0A%0A\n"
metadata_diff: {"new":{},"deleted":[]}
encryption_cipher_text: 
encryption_applied: 0
updated_time: 2020-05-26T10:44:44.809Z
created_time: 2020-05-26T10:44:44.809Z
type_: 13