Machine Learning

# MIT 6.S191: Introduction to Deep Learning
Produced by MIT OpenCourseWare	
25/05/2020 11:26
26/05/2020 23:25
Lecture 1 (Taught by Alexnader Amini)

## What is deep learning?
1. Artificial Intelligence is the ability for machines to emulate human behaviour
2. Machine Learning is a subset of AI, where machines can learn without being explicitly programmed
3. Deep Learning is a subset of ML, where machines can extract the most important pieces of information to use to train itself

## Why deep learning now?
1. Access to parallel computing through powerful Graphics Processing Units (GPUs)
2. Access to big data that can is necessary to train these algorithms
3. New software products (like tensorflow), improved and easier to use models

## 2 basic ML techniques
1. Classification: Classifying whether a fruit is a lemon or not a lemon
2. Linear Regression: Predicting grades on a test by knowing the (1) number of lectures attended and (2) the hours spent on homework (...)
 
## The perceptron
A perceptron model is one of the earliest forms of ML (1958). This technique attempts to model how neurons in our brain activate when learning new information.

There are 3 main components:
1. Input layer
2. Hidden layer
3. Output

When the multiple perceptrons are combined with each other, they form what is known as an Artifical Neural Network (ANN)

## Input layer
The various **features** of the data are fed in via the input layer, lets say $x_1, x_2$
represent 2 features, such as lectures attended $(x_1)$ and hours spent on homework ($x_2$). For the computer to understand, these are fed in as matrices.
$$
\begin{pmatrix}
4 \\
5
\end{pmatrix}
$$

Each input is initialized with a random **weight**. This shows how important that feature is for predicting the outcome. Initially, it is random, but as it learns, the weights update to reflect which features can best predict the outcome.

## Hidden layer
The layers that process the input information to predict an output are collectively known as hidden layers. Two basic types:
1. Single layer perceptron model (simplistic)
2. Multi-layer/sequential perceptron model (more complex)

Each of the hidden layers have corresponding weights that connect the neurons. Each input neuron connects an output neuron. If there are n features, the prediction for one output neuron is calculated as follows:
$$
\hat{y}=\sum_{i=1}^{n}x_iw_i
$$

Let j represent the number of output neurons. Thus, for all connected neurons (aka multi-output perceptron):
$$
\hat{y}_j=\sum_{i=1}^{n}x_iw_{i,j}
$$

## Bias term
There is an additional bias term 
$$
x_o=1
$$
which is independent to the inputs and is needed to shift the activation function (explained later) to the left or right regardless of the inputs. Thus, 

$$
\hat{y}_j=w_{0,j}+\sum_{i=1}^{n}x_iw_{i,j}
$$

## Activation function
Once all the neurons and their weights are summed up, they pass through an activation function represented as $g$

### Importance of non-linearity
Activation functions are non linear because the equation so far produces a linear equation as the prediction. As real life data is not normally linear, a linear model may not effectively predict the output. Thus, a non-linearity in the form of an activation function is applied.

### Additional utility of activation functions
The activation function can convert the prediction into something useful to be used later. A common non-linear activation function is the sigmoid function. This converts any real number into a probability between 0 and 1. Thus,

$$
\hat{y}_j=g(w_{0,j}+\sum_{i=1}^{n}x_iw_{i,j})
$$

Other non-linear activation functions include:
1. Hyperbolic tangent (tanh)
2. Rectified linear unit (relu)

### Multi-layered perceptron/Sequential model
These are typically used in deep neural networks where the layers are automatically generated. In standard ANNs, 

Let k represent the number of layers. The prediction for each layer then becomes

$$
\hat{y}_{j,k}=g(w_{0,j}^{(k)}+\sum_{i=1}^{n}x_iw_{i,j}^{(k)})
$$

## Loss 
The loss tells us how wrong the prediction was. It does this by comparing the prediction to the actual value. 

There are mathematical functions that can quantify loss. They are known typically as (but mean the same thing):
1. Cost function
2. Objective function
3. Empirical risk

and notated by:

$$
J(W)
$$

There are 2 main methods of calculating loss, depending on the problem
1. Classification problem: use Softmax cross-entropy loss
2. Regression problem: use mean-squared-error loss

## Backpropogation
This is a technique to make the machine learn. It moves backwards, from output to input and updates the weights.

## Loss optimization using Stochastic Gradient Descent
With information about the loss, the objective is to minimize the loss so that the machine can *learn* what were the best combination of weights for the corresponding features.

Minimization/optimization is a calculus problem. One technique of optimization is known as Stochastic Gradient Descent (SGD) (1952).

SGD starts with initializing the neural network with random weights, then starting at a random point.
$$ x_1, x_2 $$ 

It then partially differentiates with respect to each variable in order to find the value where the loss function is at a minumum, then moves to that point. It repeats this process until the loss function reaches a local minima.

### Batch Stochastic Gradient Descent
Since using 1 point is noisy and using all points is too computationally intensive, batches of points (e.g. 100) points can be taken and then the loss over the whole batch can be computed as an average.

### Learning rate ${\eta}$

Learning rate defines how quickly the loss function should optimize. Stochastic GD can sometimes be ineffective as it gets stuck in a local minima and cannot get out. Thus, we would never know if the lowest loss is truly the lowest loss achievable. On the other hand, if learning rate is too high, it might overshoot the local minima.

To combat this, modern loss optimization techniques take a profile about how the machine is learning including:
1. How large the gradient is
2. How fast the learning is
3. Size of weights
4. ...

There are more effective Gradient Descent techniques than SGD:
1. Adam (2014)
2. Adadelta (2012)
4. RMSProp (2011)


## Regularization

1. Underfitting: Model cannot capture the true complexity of the problem. Too generalized
2. Overfitting: Model starts memorizing features training data and cannot generalize

Regularization is a process of preventing overfitting. This can help better generalize.

There are some techniques of regularization
1. Dropout: Randomly not activating some neurons, so the network cannot remember the data through pre-defined channels
2. Early stopping: Monitoring training and test set. When test set loss starts diversging from training set, stop training to prevent overfitting but keep the lowest loss so that there is no underfit.






















id: 989faf8706574a4b8719e7d3764c1473
parent_id: c531fb3a1321454ab0a3a58decb0bf67
created_time: 2020-05-26T03:24:52.327Z
updated_time: 2020-05-28T11:16:54.625Z
is_conflict: 0
latitude: 0.00000000
longitude: 0.00000000
altitude: 0.0000
author: 
source_url: 
is_todo: 0
todo_due: 0
todo_completed: 0
source: joplin-desktop
source_application: net.cozic.joplin-desktop
application_data: 
order: 0
user_created_time: 2020-05-26T03:24:52.327Z
user_updated_time: 2020-05-28T11:16:54.625Z
encryption_cipher_text: 
encryption_applied: 0
markup_language: 1
is_shared: 0
type_: 1