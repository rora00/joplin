id: 4cd81b80ccd242c88fec2f6974a3a172
parent_id: 97f83a7df80e40058b0fa8e90f5c3608
item_type: 1
item_id: 989faf8706574a4b8719e7d3764c1473
item_updated_time: 1590492277753
title_diff: 
body_diff: "@@ -4502,25 +4502,192 @@\n %0A## \n-Loss optimization\n+Backpropogation%0AThis is a technique to make the machine learn. It moves backwards, from output to input and updates the weights.%0A%0A## Loss optimization using Stochastic Gradient Descent\n %0AWit\n@@ -4877,23 +4877,9 @@\n tion\n- also known as \n+/\n opti\n@@ -5318,17 +5318,16 @@\n function\n-r\n  reaches\n@@ -5344,17 +5344,76 @@\n inima.%0A%0A\n-S\n+### Learning rate%0A%0A**learning rate** ($%7B%5Ceta%7D$)%0A%0AStochastic \n GD can s\n@@ -5518,39 +5518,129 @@\n r kn\n-w%0A%0A## Backpropogation%0AThis is a\n+ow if the lowest loss is truly the lowest loss achievable. On the other hand, %0A%0A%0ATo combat this, modern loss optimization\n  tec\n@@ -5641,33 +5641,51 @@\n on technique\n+s\n  t\n-o m\n ake \n+a profile about how \n the machine \n@@ -5688,110 +5688,232 @@\n ine \n+is \n learn\n-. It moves backwards, from output to input and updates the weights based on the g\n+ing including:%0A1. How large the gradient is%0A2. How fast the learning is%0A3. Size of weights%0A4. ...%0A%0AThus, there are more effective G\n radient \n-d\n+D\n escent\n-.\n+ techniques:%0A1. Adam (2014)%0A2. Adadelta (2012)%0A4. RMSProp (2011)%0A%0A%0A%0A%0A%0A\n %0A%0A%0A%0A\n"
metadata_diff: {"new":{},"deleted":[]}
encryption_cipher_text: 
encryption_applied: 0
updated_time: 2020-05-26T11:24:44.891Z
created_time: 2020-05-26T11:24:44.891Z
type_: 13