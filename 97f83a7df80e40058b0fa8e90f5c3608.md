id: 97f83a7df80e40058b0fa8e90f5c3608
parent_id: 998aac4d1de544afb7058501e18b9ea8
item_type: 1
item_id: 989faf8706574a4b8719e7d3764c1473
item_updated_time: 1590491684664
title_diff: 
body_diff: "@@ -4285,16 +4285,1203 @@\n l risk%0A%0A\n+and notated by:%0A%0A$$%0AJ(W)%0A$$%0A%0AThere are 2 main methods of calculating loss, depending on the problem%0A1. Classification problem: use Softmax cross-entropy loss%0A2. Regression problem: use mean-squared-error loss%0A%0A## Loss optimization%0AWith information about the loss, the objective is to minimize the loss so that the machine can *learn* what were the best combination of weights for the corresponding features.%0A%0AMinimization also known as optimization is a calculus problem. One technique of optimization is known as Stochastic Gradient Descent (SGD) (1952).%0A%0ASGD starts with initializing the neural network with random weights, then starting at a random point.%0A$$ x_1, x_2 $$ %0A%0AIt then partially differentiates with respect to each variable in order to find the value where the loss function is at a minumum, then moves to that point. It repeats this process until the loss functionr reaches a local minima.%0A%0ASGD can sometimes be ineffective as it gets stuck in a local minima and cannot get out. Thus, we would never knw%0A%0A## Backpropogation%0AThis is a technique to make the machine learn. It moves backwards, from output to input and updates the weights based on the gradient descent.%0A%0A%0A%0A%0A\n %0A%0A%0A%0A%0A%0A%0A%0A\n"
metadata_diff: {"new":{},"deleted":[]}
encryption_cipher_text: 
encryption_applied: 0
updated_time: 2020-05-26T11:14:44.875Z
created_time: 2020-05-26T11:14:44.875Z
type_: 13