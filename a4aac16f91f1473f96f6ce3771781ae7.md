id: a4aac16f91f1473f96f6ce3771781ae7
parent_id: 4cd81b80ccd242c88fec2f6974a3a172
item_type: 1
item_id: 989faf8706574a4b8719e7d3764c1473
item_updated_time: 1590492800763
title_diff: 
body_diff: "@@ -5352,26 +5352,251 @@\n ### \n-Learning rate%0A%0A**l\n+Batch Stochastic Gradient Descent%0ASince using 1 point is noisy and using all points is too computationally intensive, batches of points (e.g. 100) points can be taken and then the loss over the whole batch can be computed as an average.%0A%0A### L\n earn\n@@ -5607,23 +5607,88 @@\n rate\n-** (\n+ \n $%7B%5Ceta%7D$\n-)\n %0A%0A\n+Learning rate defines how quickly the loss function should optimize. \n Stoc\n@@ -5882,17 +5882,82 @@\n r hand, \n-%0A\n+if learning rate is too high, it might overshoot the local minima.\n %0A%0ATo com\n@@ -6152,14 +6152,8 @@\n %0A%0ATh\n-us, th\n ere \n@@ -6198,16 +6198,25 @@\n chniques\n+ than SGD\n :%0A1. Ada\n@@ -6263,16 +6263,693 @@\n 2011)%0A%0A%0A\n+## Underfitting and overfitting%0A%0AUnderfitting: Model cannot capture the true complexity of the problem. Too generalized%0A%0AOverfitting: Model starts memorizing features training data and cannot generalize%0A%0A### Regularization%0A%0ARegularization is a process of preventing overfitting. This can help better generalize.%0A%0AThere are some techniques of regularization%0A1. Dropout: Randomly not activating some neurons, so the network cannot remember the data through pre-defined channels%0A2. Early stopping: Monitoring training and test set. When test set loss starts diversging from training set, stop training to prevent overfitting but keep the lowest loss so that there is no underfit.%0A\n %0A%0A%0A%0A%0A%0A%0A%0A\n"
metadata_diff: {"new":{},"deleted":[]}
encryption_cipher_text: 
encryption_applied: 0
updated_time: 2020-05-26T11:34:46.875Z
created_time: 2020-05-26T11:34:46.875Z
type_: 13